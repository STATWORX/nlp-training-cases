{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Train a Deep Haiku Generator\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting working dir to: /Users/ingomarquart/Documents/GitHub/itern-nlp-training-cases\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "print(f'Setting working dir to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Fine-tuning for Haiku Generation\n",
    "\n",
    "In this notebooks, we will start fine-tuning a GPT-style causal language model for Haiku generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Data Loading\n",
    "\n",
    "Use the `load_dataset` function from the `datasets` package to import the two files *haiku_data_1.txt* and *haiku_data_2.txt* from the *data* folder.\n",
    "Check out the structure of the newly created object.   \n",
    "\n",
    "The `Dataset` and `DatasetDict` classes come with a `.train_test_split()` method for splitting data into train and test sets.\n",
    "Use it to create two datasets, one for train, the other for test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion = 0.1\n",
    "\n",
    "# Add your solution here:\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b0395c5cfc92a962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /Users/ingomarquart/.cache/huggingface/datasets/text/default-b0395c5cfc92a962/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013993978500366211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581e26cfbe174bd8a36cb8128b5d29f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015959978103637695,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45f76c600fa41be999e6c7bfc920be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015275001525878906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": " tables",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b547f27a6584e3eb76cdaec25e5ff8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/ingomarquart/.cache/huggingface/datasets/text/default-b0395c5cfc92a962/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011512994766235352,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24724a352ca4adda244d26a9f21aba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010336160659790039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 112,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45b7140ad4e42d9abca22fcc5238eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/112 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loading the dataset, it looks like:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 89119\n",
      "    })\n",
      "})\n",
      "After splitting the dataset, it  looks like:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 80207\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8912\n",
      "    })\n",
      "})\n",
      "The training set looks like:\n",
      "{'text': ['none of the students', 'how to tell', '<|endoftext|>', 'gone to seed', 'the sea', 'spring morning', 'the cactus flower', 'memory betrays', '<|endoftext|>', 'on a foggy window']}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset using load_dataset and the \"text\" argument\n",
    "files = ['data/haiku_data_1.txt', 'data/haiku_data_2.txt']\n",
    "dataset_raw = load_dataset('text', data_files=files)\n",
    "dataset_raw = dataset_raw.filter(lambda x: x['text'] != '')\n",
    "\n",
    "print('After loading the dataset, it looks like:')\n",
    "print(dataset_raw)\n",
    "\n",
    "# Apply the train_test_split method to create two new datasets\n",
    "dataset_raw = dataset_raw['train'].train_test_split(test_proportion)\n",
    "\n",
    "print('After splitting the dataset, it  looks like:')\n",
    "print(dataset_raw)\n",
    "\n",
    "# Slice a few samples and take a look at it\n",
    "print('The training set looks like:')\n",
    "print(dataset_raw['train'][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Pre-Processing\n",
    "\n",
    "Next, we need to pre-process our data and namely tokenize it.\n",
    "\n",
    "- Load the correct pre-trained tokenizer for your model\n",
    "\n",
    "As you have probability noticed, the raw text files already contain an EOS-token in the form of \"<|endoftext|>\".   \n",
    "- Check out, what kind of EOS-token the loaded model expects\n",
    "- Change the EOS-token if necessary\n",
    "\n",
    "Now comes the tricky part, we need to tokenize and chunck the data for the model training.   \n",
    "- Define two separate functions, one for tokenization and one for chunking  \n",
    "- Then apply both to the dataset using the `.map()` method\n",
    "- Define a data collator using the `DataCollatorForLanguageModeling` class   \n",
    "- Test the whole pipeline by drawing a sample for the tokenized and chunked dataset and feed it through the data collator for batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "block_size = 128\n",
    "\n",
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the tokenizer function, you just need to tokenizer the entire dataset\n",
    "- For the chunking function, concatenate the various inputs to a long list\n",
    "- Afterwards, split the list using the block size defined below\n",
    "- It should be fine to drop any reminder that does not fit into the last context windows\n",
    "- For the label IDs, you can just copy the input IDs (the shifting will be handled later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print('Context length:', tokenizer.model_max_length)\n",
    "\n",
    "# Check EOS token\n",
    "print(f'Model special tokens:\\n {tokenizer.special_tokens_map}')\n",
    "\n",
    "# We will use two separat functions for tokenization and chunking\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "# Function to chunk the text\n",
    "def chunk_function(examples):\n",
    "    \n",
    "    # Concatenate all texts via sum of lists\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # We drop the small remainder, we could add padding if the model supported \n",
    "    # it instead of this drop, you can customize this part to your needs\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of max_len size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Later DataCollatorForLanguageModeling will handling the input, therefore \n",
    "    # just copy inputs to labels\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply tokenize and chunking to texts in the dataset\n",
    "# batched=True for faster computation \n",
    "# remove_columns because we don't need the raw text for training anymore\n",
    "dataset_tokenized = dataset_raw.map(tokenize_function, \n",
    "                                    batched=True, \n",
    "                                    remove_columns=['text'])\n",
    "\n",
    "dataset_lm = dataset_tokenized.map(chunk_function,\n",
    "                                   batched=True)\n",
    "\n",
    "# Create the DataCollator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Setting the pad_token to the eos_token is the default for GPT-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Test what we have ceated\n",
    "# First we draw a single sample from the created dataset\n",
    "sample = dataset_lm['train'][0]\n",
    "print(f'First 5 elements of a sample :\\n {sample[\"input_ids\"][0:5]}')\n",
    "\n",
    "# Then we batch the example (data_collator expects a list of dicts)\n",
    "batch = data_collator([sample])\n",
    "\n",
    "# And take a look of what comes out\n",
    "print(f'Batch has {batch.keys()}')\n",
    "print(f'input_ids = {batch[\"input_ids\"].shape} \\n \\\n",
    "        attention_mask = {batch[\"attention_mask\"].shape} \\n \\\n",
    "        labels = {batch[\"labels\"].shape}')\n",
    "\n",
    "print(batch['input_ids'][0, :5].detach().cpu().numpy())\n",
    "print(batch['labels'][0, :5].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Model Training\n",
    "\n",
    "After pre-processing our data, we can now fine-tune our pre-trained GPT-2 model. We will use the `AutoModelForCausalLM` and `Trainer` classes from the ðŸ¤— package to do so.  \n",
    "\n",
    "- Load the pre-trained model and place it on the right device\n",
    "- Define appropriate training arguments\n",
    "- Define a (PyTorch) Trainer\n",
    "- Start the training ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../checkpoints\n",
    "!mkdir -p ../logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder for checkpoints / logs\n",
    "output_dir = f'../checkpoints/{model_name}'\n",
    "log_dir = f'../logs/{model_name}'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Load model an place on appropriate device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    logging_dir=log_dir,\n",
    "    num_train_epochs=3,\n",
    "    # max_steps=150000, use as alternative to epochs, \n",
    "    # but make sure to change evaluation_strategy='steps' \n",
    "    # and eval_steps=1000 accordingly\n",
    "    warmup_steps=1000,\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_lm['train'],\n",
    "    eval_dataset=dataset_lm['test'],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start the training and save model after training\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Generate from Model\n",
    "\n",
    "Try to see how the model generates haiku from the given seed text. Make sure to also play around with the decoding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promot for model input\n",
    "new_input_txt = 'Deep learning'\n",
    "\n",
    "# Create tokens\n",
    "input_ids = tokenizer.encode(new_input_txt, return_tensors='pt').to(device)\n",
    "\n",
    "# Run forwardpass\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids, max_length=25)\n",
    "\n",
    "# Decode the output\n",
    "tokenizer.decode(output_ids[0].detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp-training')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c662d7cd23b6de128b6d84794298b91bf8fa078dbbac08b1f8f98d16f1457de4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
