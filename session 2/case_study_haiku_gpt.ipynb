{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Train a Deep Haiku Generator\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting working dir to: /Users/ingomarquart/Documents/GitHub/itern-nlp-training-cases\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "print(f'Setting working dir to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Fine-tuning for Haiku Generation\n",
    "\n",
    "In this notebooks, we will start fine-tuning a GPT-style causal language model for Haiku generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Data Loading\n",
    "\n",
    "Use the `load_dataset` function from the `datasets` package to import the two files *haiku_data_1.txt* and *haiku_data_2.txt* from the *data* folder.\n",
    "Check out the structure of the newly created object.   \n",
    "\n",
    "The `Dataset` and `DatasetDict` classes come with a `.train_test_split()` method for splitting data into train and test sets.\n",
    "Use it to create two datasets, one for train, the other for test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_proportion = 0.1\n",
    "\n",
    "# Add your solution here:\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Pre-Processing\n",
    "\n",
    "Next, we need to pre-process our data and namely tokenize it.\n",
    "\n",
    "- Load the correct pre-trained tokenizer for your model\n",
    "\n",
    "As you have probability noticed, the raw text files already contain an EOS-token in the form of \"<|endoftext|>\".   \n",
    "- Check out, what kind of EOS-token the loaded model expects\n",
    "- Change the EOS-token if necessary\n",
    "\n",
    "Now comes the tricky part, we need to tokenize and chunck the data for the model training.   \n",
    "- Define two separate functions, one for tokenization and one for chunking  \n",
    "- Then apply both to the dataset using the `.map()` method\n",
    "- Define a data collator using the `DataCollatorForLanguageModeling` class   \n",
    "- Test the whole pipeline by drawing a sample for the tokenized and chunked dataset and feed it through the data collator for batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "block_size = 128\n",
    "\n",
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For the tokenizer function, you just need to tokenizer the entire dataset\n",
    "- For the chunking function, concatenate the various inputs to a long list\n",
    "- Afterwards, split the list using the block size defined below\n",
    "- It should be fine to drop any reminder that does not fit into the last context windows\n",
    "- For the label IDs, you can just copy the input IDs (the shifting will be handled later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Model Training\n",
    "\n",
    "After pre-processing our data, we can now fine-tune our pre-trained GPT-2 model. We will use the `AutoModelForCausalLM` and `Trainer` classes from the ðŸ¤— package to do so.  \n",
    "\n",
    "- Load the pre-trained model and place it on the right device\n",
    "- Define appropriate training arguments\n",
    "- Define a (PyTorch) Trainer\n",
    "- Start the training ðŸ™ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../checkpoints\n",
    "!mkdir -p ../logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Generate from Model\n",
    "\n",
    "Try to see how the model generates haiku from the given seed text. Make sure to also play around with the decoding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here:\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp-training')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c662d7cd23b6de128b6d84794298b91bf8fa078dbbac08b1f8f98d16f1457de4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
