{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MoymJFDKk94"
      },
      "source": [
        "NLP Training 6: Attention from scratch\n",
        "--- \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YOJNSxXaKQFs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting working dir to: /Users/ingomarquart/Documents/GitHub\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import os\n",
        "\n",
        "os.chdir('..')\n",
        "print(f'Setting working dir to: {os.getcwd()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE1BTiuT9BwT"
      },
      "source": [
        "## Writing an attention layer from scratch with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8By7yUxHLkgw"
      },
      "source": [
        "### Overview\n",
        "\n",
        "We will write one attention layer with a single attention head from scratch, to illustrate how a fairly complex model can be written and composed in PyTorch.\n",
        "\n",
        "\n",
        "For the attention head, we need the following components\n",
        "\n",
        "1.   A scaled dot product\n",
        "2.   Query, Key and Value operations\n",
        "\n",
        "and to write a single-head attention layer, we also need\n",
        "\n",
        "3.   Layer Norm\n",
        "4.   Feedforward Network\n",
        "5.   An attention module that implements the above, plus skip connections\n",
        "\n",
        "\n",
        "We will implement all these at PyTorch modules to get the hang of it.\n",
        "In practice, we can instead define functions and use only one or two modules.\n",
        "\n",
        "\n",
        "First, let's write our first PyTorch Module\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtBC71RQNeG0"
      },
      "source": [
        "### Exercise 1: Implementing a PyTorch Module\n",
        "\n",
        "PyTorch modules need to implement a couple of methods:    \n",
        "- First, `__init()__`, which also needs to call the superclass's initialization.    \n",
        "- Second, a `forward` method that tells PyTorch what takes the module from input to output.\n",
        "\n",
        "Create a new class called `TestModule` that inherits from `nn.Module` and implements a `__init__` and `__forward__` method. The later should just print `Hello World`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i93oOAfLK8Z6"
      },
      "outputs": [],
      "source": [
        "class TestModule(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self):\n",
        "    print(\"Hello World\")\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEAbsf8A9XYV"
      },
      "source": [
        "Different from other ML frameworks, PyTorch modules act like simple Python classes. Test it by instantiate your class an call it just like any other Python module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwQ6C0BULKki",
        "outputId": "687bfa9d-e644-4e71-9693-eee163bd4735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the class\n",
        "test_module = TestModule()\n",
        "\n",
        "# We call the forward pass by calling the module directly!\n",
        "test_module()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsUvWe5gY11a"
      },
      "source": [
        "### Exercise 2:  Query, Key, and Value Operation\n",
        "\n",
        "Let's start with our first real module. We will code the operation that duplicates our input embedding vector `X` three times and transforms them separately into key, query and value vectors.\n",
        "\n",
        "The Query, Key and Value operation is a simple matrix multiplication and doesn't need its own module. \n",
        "\n",
        "But since it is simple, we will write this as a module (`torch.nn.Module)`). On top of that, we will even write two of the three multiplications (for Query and Key) by hand. This goes to show that PyTorch can keep arbitrary variables in the computational graph (not just modules or prefixed functions) and optimize them - provided that we register them correctly. For the Values we will use `nn.Linear` to do the multiplication.\n",
        "\n",
        "We already created a model skeleton for you, try to fill it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-YXxgaS8O0EJ"
      },
      "outputs": [],
      "source": [
        "class QueryKeyValue(nn.Module):\n",
        "  def __init__(self, dim_h, dim_k):\n",
        "    \"\"\"\n",
        "    dim_n: Hidden embedding dimensions of the inputs\n",
        "    dim_k: Embedding dimension of outputs going into the attention mechanism\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Here we write the parameter matrices ourselves for Query and Key\n",
        "    # Since we use torch.ones it will automatically filled with 1s\n",
        "    # For real use cases we would use torch.empty and a specific initialization function\n",
        "    self.W_q = nn.Parameter(torch.ones([dim_h, dim_k], requires_grad = True))\n",
        "    self.W_k = nn.Parameter(torch.ones([dim_h, dim_k], requires_grad = True))\n",
        "    # Now we need to register the parameters with the module\n",
        "    # Otherwise, PyTorch would not optimize them in the backwards pass\n",
        "    self.register_parameter('W_q', self.W_q)\n",
        "    self.register_parameter('W_k', self.W_k)\n",
        "\n",
        "    # For value, we do it using a PyTorch Module (a linear layer)\n",
        "    # This is equivalent for the solution above, but mich shorter\n",
        "    # We do not need to register the parameters, since they are already registered\n",
        "    # Same for the initialization of weights\n",
        "    self.value = nn.Linear(dim_h, dim_k)\n",
        "\n",
        "  def forward(self, X):\n",
        "    # Our custom linear functions\n",
        "    queries = torch.matmul(X, self.W_q)\n",
        "    keys = torch.matmul(X, self.W_k)\n",
        "    # Torch Module\n",
        "    values = self.value(X)\n",
        "    # And we return\n",
        "    return queries, keys, values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3 - Test your Module\n",
        "\n",
        "After the implementation, we need to the the Module:\n",
        "\n",
        "- Let's create a test tensor with two observations and the embedding dimension 5. Since our parameters will be `torch.floats`, we need to specify the same datatype, otherwise `matmul` will throw an error.\n",
        "- Let us initialize a `QueryKeyValue` class. `dim_h` needs to be 5, which is the dimension of our tensor. By contrast, we can choose the dimension for the attention embeddings. In BERT, it will be `dim_h` divided by the number of heads. Here, let's do 3.\n",
        "- If we plug in our test tensor, we should get three outputs, all with the same size, can you confirm?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sUtmDlk48kj6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "# A simple tensor for test purposes\n",
        "test_tensor = torch.tensor([[1, 2, 3, 4, 5], \n",
        "                            [5, 6, 7, 8, 10]], dtype=torch.float32)\n",
        "\n",
        "# Init our Module\n",
        "qkv_mechanism = QueryKeyValue(5, 3)\n",
        "\n",
        "# Run a pass through the model\n",
        "queries,keys,values = qkv_mechanism(test_tensor)\n",
        "print(queries.shape)\n",
        "print(keys.shape)\n",
        "print(values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Sj7h4f_r0G"
      },
      "source": [
        "Since we have initialized our own parameters (for queries and keys) as one, we can also reason about what the output should be.\n",
        "First, they should be equal for queries and keys (both weights are one).\n",
        "\n",
        "Second, since weights are one, the first row should be\n",
        "$$\n",
        "[1,2,3,4,5] * [1,1,1,1,1]^T$$\n",
        "that is\n",
        "$$\n",
        "1*1+1*2+1*3+1*4+1*5 = 15\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqt437j67rQH",
        "outputId": "8e9c219e-1c04-4bfd-8327-7a52173c3f9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[15., 15., 15.],\n",
            "        [36., 36., 36.]], grad_fn=<MmBackward0>)\n",
            "tensor([[15., 15., 15.],\n",
            "        [36., 36., 36.]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(queries)\n",
        "print(keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1ayPi77AWcs"
      },
      "source": [
        "By contrast, the values module is initialized by PyTorch - it has random weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU_9iwAH78Nu",
        "outputId": "3e03573a-c73a-4019-a925-4b9ef635e136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.9126,  0.5544, -3.0775],\n",
            "        [ 2.5894,  2.3251, -8.2189]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l38VEVJkPVig"
      },
      "source": [
        "### Exercise 4: Scaled Dot Product\n",
        "\n",
        "For our scaled dot product, we take three matrices and compute attention scores and output embeddings. This time, we will use PyTorch build in modules - specifically, the softmax module.\n",
        "\n",
        "A second option is to use PyTorch functional API: softmax is a simple function, and PyTorch can keep track of its impact on the gradients. In this case, we include `torch.functional` as `F` and employ `F.softmax()`.\n",
        "\n",
        "As before, try to fill in the missing parts in the Module skeleton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E70oIkTIYxrO"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProduct(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Specify dim for clarity - torch will compute over the last dimension by default.\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    \"\"\"\n",
        "    Q,K and V we have computed earlier\n",
        "    \"\"\"\n",
        "    # Dot Product - allowing for a batch dimension\n",
        "    attention = torch.matmul(Q, K.T)\n",
        "    # Get the normalization constant. We need the embedding size of the attention layer\n",
        "    # aka the last dimension of each of the matrices\n",
        "    dim_k = Q.shape[-1]\n",
        "    # attention scores correspond to the softmax of the normed matrix\n",
        "    attention = self.softmax(attention / torch.sqrt(torch.tensor(2.0 * dim_k)))\n",
        "    return torch.matmul(attention, V)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR0Bk3RgQM_d"
      },
      "source": [
        "### Exercise 4:  Attention Head\n",
        "\n",
        "That's all we need for a single attention head.\n",
        "\n",
        "We will also encode this as a module, to show how nicely PyTorch composes modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MBUQodJgP3tz"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, dim_h, dim_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # Here we simply instantiate our two classes\n",
        "    self.scaled_dot_product = ScaledDotProduct()\n",
        "    self.qkv_mechanism = QueryKeyValue(dim_h, dim_k)\n",
        "\n",
        "  def forward(self,X):\n",
        "    queries, keys, values = self.qkv_mechanism(X)\n",
        "    X_next_layer = self.scaled_dot_product(queries, keys, values)\n",
        "\n",
        "    return X_next_layer\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntK0FiyARn0w"
      },
      "source": [
        "Let's test it!\n",
        "\n",
        "Since we only have one head, and no concat operation, we would want to set `dim_h=dim_k`. After all, the input and output dimension of a transformer layer should be the same!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_tensor = torch.tensor([[1, 2, 3, 4, 5], \n",
        "                            [5, 6, 7, 8, 10]], dtype=torch.float32)\n",
        "\n",
        "# Add your solution here:\n",
        "# ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyJuZurfREBM",
        "outputId": "44dbedde-576e-4264-931b-697df1fa08cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.8646, -1.3807, -0.7563, -0.9157, -2.6003],\n",
              "        [ 0.8646, -1.3807, -0.7563, -0.9157, -2.6003]], grad_fn=<MmBackward0>)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention_head = AttentionHead(5, 5)\n",
        "attention_head(test_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94KqQ795UdWf"
      },
      "source": [
        "### Exercise 5:  Recap - and Batching\n",
        "\n",
        "We have so far supplied a test tensor with two dimensions. The first dimension is the token, the second the embedding dimension.\n",
        "Thus, we can train one sentence at a time\n",
        "\n",
        "This would take a long time... the real benefit of a transformer is that we can train many sequences (batches) in parallel.\n",
        "\n",
        "To test this, we just add another \"batch\" to our test tensor - the same sequence duplicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hN5Aku-TjN7",
        "outputId": "081a97df-472e-4b78-eeab-a6b482e2d3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 5])\n",
            "tensor([[[ 1.,  2.,  3.,  4.,  5.],\n",
            "         [ 5.,  6.,  7.,  8., 10.]],\n",
            "\n",
            "        [[ 1.,  2.,  3.,  4.,  5.],\n",
            "         [ 5.,  6.,  7.,  8., 10.]]])\n"
          ]
        }
      ],
      "source": [
        "test_tensor = torch.tensor([[[1, 2, 3, 4, 5], [5, 6, 7, 8, 10]], \n",
        "                            [[1, 2, 3, 4, 5], [5, 6, 7, 8, 10]]], dtype=torch.float32)\n",
        "print(test_tensor.shape)\n",
        "print(test_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg_vp_9mVSJh"
      },
      "source": [
        "If we were to run this on our above attention layer, it would fail, but we only need to change one thing:\n",
        "\n",
        "In the QKV mechanism we should unsqueeze our weight matrices to give them a batch dimension (which is empty). matmul will then broadcast the matrices and apply them to each batch. For the Linear Layer (`nn.Linear`), we of course don't need to worry about it.\n",
        "\n",
        "In addition, our Scaled Dot Product now works across batches. We have the same number of batches for all matrices - hence no need to broadcast.\n",
        "In that case, `torch.bmm` (batched matrix multiplication) is faster!\n",
        "\n",
        "Previously, we have transposed the Key vector. Of course, we can not transpose the batch dimension. Instead of K.T, we will specify that we only want to transpose the last two dimensions, using `.transpose(-1,-2)` method.\n",
        "\n",
        "The two places are marked by #!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nIU83QTrVziR"
      },
      "outputs": [],
      "source": [
        "class QueryKeyValue(nn.Module):\n",
        "  def __init__(self, dim_h, dim_k):\n",
        "    \"\"\"\n",
        "    dim_n: Hidden embedding dimensions of the inputs\n",
        "    dim_k: Embedding dimension of outputs going into the attention mechanism\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Here we write the parameter matrices ourselves for Query And Key\n",
        "    self.W_q = nn.Parameter(torch.ones([dim_h, dim_k], requires_grad = True))\n",
        "    self.W_k = nn.Parameter(torch.ones([dim_h, dim_k], requires_grad = True))\n",
        "    # Now we need to register the parameters with the module\n",
        "    # Otherwise, PyTorch would not optimize them in the backwards pass\n",
        "    self.register_parameter('W_q',self.W_q)\n",
        "    self.register_parameter('W_q',self.W_q)\n",
        "    # Finally, we also need to initialize the parameters\n",
        "\n",
        "    # For value, we do it using a PyTorch Module\n",
        "    self.value = nn.Linear(dim_h,dim_k)\n",
        "    \n",
        "    # \n",
        "\n",
        "  def forward(self, X):\n",
        "    # Our custom linear functions\n",
        "    # !!\n",
        "    queries = torch.matmul(X,self.W_q.unsqueeze(0))\n",
        "    keys = torch.matmul(X,self.W_k.unsqueeze(0))\n",
        "    # Torch Module\n",
        "    values = self.value(X)\n",
        "    # And we return\n",
        "    return queries, keys, values\n",
        "\n",
        "class ScaledDotProduct(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Specify dim for clarity - torch will compute over the last dimension by default.\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    \"\"\"\n",
        "    Q,K and V we have computed earlier\n",
        "    \"\"\"\n",
        "    # Dot Product - allowing for a batch dimension - here we used batched matrix multiply with transpose\n",
        "    # !!\n",
        "    attention = torch.bmm(Q,K.transpose(-1,-2))\n",
        "    # Get the normalization constant. We need the embedding size of the attention layer\n",
        "    # aka the last dimension of each of the matrices\n",
        "    dim_k = Q.shape[-1]\n",
        "    # attention scores correspond to the softmax of the normed matrix\n",
        "    attention = self.softmax(attention / torch.sqrt(torch.tensor(2.0*dim_k)))\n",
        "    return torch.matmul(attention, V)\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, dim_h, dim_k):\n",
        "    super().__init__()\n",
        "\n",
        "    # Here we simply instantiate our two classes\n",
        "    self.scaled_dot_product = ScaledDotProduct()\n",
        "    self.qkv_mechanism = QueryKeyValue(dim_h, dim_k)\n",
        "\n",
        "  def forward(self,X):\n",
        "    queries, keys, values = self.qkv_mechanism(X)\n",
        "    X_next_layer = self.scaled_dot_product(queries, keys, values)\n",
        "\n",
        "    return X_next_layer\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q04LqUDhVxy5"
      },
      "source": [
        "Let's test it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_tensor = torch.tensor([[[1, 2, 3, 4, 5], [5, 6, 7, 8, 10]],\n",
        "                            [[1, 2, 3, 4, 5], [5, 6, 7, 8, 10]]], dtype=torch.float32)\n",
        "\n",
        "# Add your solution here:\n",
        "# ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOvni0lrWOrC",
        "outputId": "1271aac7-08b6-43d1-8286-1a8d41acf299"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-3.2694, -1.4893, -1.7046, -1.1541,  0.5490],\n",
              "         [-3.2694, -1.4893, -1.7046, -1.1541,  0.5490]],\n",
              "\n",
              "        [[-3.2694, -1.4893, -1.7046, -1.1541,  0.5490],\n",
              "         [-3.2694, -1.4893, -1.7046, -1.1541,  0.5490]]],\n",
              "       grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attention_head = AttentionHead(5,5)\n",
        "attention_head(test_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvcpXtrwR55S"
      },
      "source": [
        "### Exercise 6:  LayerNorm (and BatchNorm)\n",
        "\n",
        "We will not code this ourselves, but we can use PyTorch to understand LayerNorm and BatchNorm better\n",
        "\n",
        "LayerNorm computes\n",
        "$$\n",
        "y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
        "$$\n",
        "\n",
        "the important point here over what dimensions the expected value and variance are computed.\n",
        "\n",
        "In our application, the first dimension of our tensor is the batch dimension, the second the dimension of observations, and the final the hidden dimension.\n",
        "\n",
        "LayerNorm as applied in BERT norms across the hidden dimension.\n",
        "\n",
        "let's see this on our test tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36Xq3iP2RM8r",
        "outputId": "4fe80f82-113e-45d9-cb1f-2dbd18145da0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.4142, -0.7071,  0.0000,  0.7071,  1.4142],\n",
              "        [-0.5006, -0.5003, -0.5000,  2.0000, -0.4991]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ln = nn.LayerNorm(5)\n",
        "test_tensor = torch.tensor([[[1, 2, 3, 4, 5], [5, 6, 7, 8222, 10]], \n",
        "                            [[1, 2, 22223, 4, 5], [5, 6, 7, 8, 10]]], dtype=torch.float32)\n",
        "# Consider the first batch\n",
        "ln(test_tensor[0,...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jONbs7jLdEMQ"
      },
      "source": [
        "It is a bit hard to see, but if you sum all hidden embeddings for each dimension, you get roughly 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKNfiEhHRR9t",
        "outputId": "e7492c24-6c9c-4662-8f14-0d2bdb43c541"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-3.5763e-07],\n",
              "        [-2.3842e-07]], grad_fn=<SumBackward1>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ln(test_tensor[0,...]).sum(-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4xGIh41e6yR"
      },
      "source": [
        "If we instead sum up across observations (or even batches), this is not true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uECa5yHwSPXx",
        "outputId": "473af204-9721-4911-debc-aab86004af28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.9148, -1.2074, -0.5000,  2.7071,  0.9151]], grad_fn=<SumBackward1>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ln(test_tensor[0,...]).sum(0, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPmoKbi9fSrH"
      },
      "source": [
        "We can compare this to the BatchNorm. Of course, we have a sequence model, where a \"batch\" corresponds to one sequence.\n",
        "\n",
        "So, for the first sequence, the batch norm gives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm5Y-PzAewu9",
        "outputId": "f34f5335-0c27-4bf6-b1f0-b7cb6d0ee16e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
              "        [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000]],\n",
              "       grad_fn=<NativeBatchNormBackward0>)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bn = nn.BatchNorm1d(5)\n",
        "bn(test_tensor[0,...])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Um6lvFgq4p"
      },
      "source": [
        "Now, the hidden dimension for each observations do not sum up to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykedhRzwfFoc",
        "outputId": "a70d5b46-7b99-4844-c549-ad5e7e5bb643"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-5.0000],\n",
              "        [ 5.0000]], grad_fn=<SumBackward1>)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bn(test_tensor[0,...]).sum(-1, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L1hOhAvgwa2"
      },
      "source": [
        "But, across observations (\"batches\"), each dimension comes out to zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQik-UdhfHos",
        "outputId": "88dcb573-04dc-4c61-e270-fb382cda61d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.1921e-07, -5.9605e-08,  0.0000e+00,  0.0000e+00, -2.3842e-07]],\n",
              "       grad_fn=<SumBackward1>)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bn(test_tensor[0,...]).sum(0, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRqIIsUNrwuH"
      },
      "source": [
        "But recall that we do not wish to add skewed relationships between observations (so, tokens) in the transformer just due to normalizations. Hence, we choose LayerNorm for our Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUNxLWehg6Wk"
      },
      "source": [
        "### Exercise 7: Feedforward Layer\n",
        "\n",
        "\n",
        "The attention mechanism is responsible for encoding complex relationships between tokens (in ML lingo we say it learns a dyadic approximation to any permutation invariant function).\n",
        "\n",
        "The Feedforward Layer is a far less exciting module of the Transformer - it is simply a set of linear regressions (`nn.Linear)`) followed by a non-linear activation function - also called a multilayer perceptron (MLP). \n",
        "\n",
        "Nevertheless, this part of the model has been identified as crucial component, as it provides expressivity to process the information from the attention layer. To do this well, the Feedfoward layer should first increase the dimensionality of the embeddings, before squashing them back down to the original size.\n",
        "\n",
        "And to have a useful composition of the two layers, instead of a plain ol' matrix multiplication, we use a non-linear activation (in this case, a GeLU `nn.GELU`).\n",
        "\n",
        "For this part, we will use standard PyTorch functions and modules, having done the extra legwork above.   \n",
        "\n",
        "See if you can fill the missing parts in the module skeleton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForwardLayer(torch.nn.Module):\n",
        "  def __init__(self, dim_h, dim_expanded):\n",
        "    \"\"\"\n",
        "    dim_h: Hidden embedding dimensions of the inputs\n",
        "    dim_expanded: \n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # from dim_h -> dim_expanded\n",
        "    self.firstlayer = nn.Linear(in_features=dim_h, out_features=dim_expanded)\n",
        "    # and back\n",
        "    self.secondlayer = nn.Linear(in_features = dim_expanded, out_features = dim_h)\n",
        "    self.activation = nn.GELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.firstlayer(x)\n",
        "    x = self.activation(x)\n",
        "    # Dropout here\n",
        "    x = self.secondlayer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO0Lva225_4C"
      },
      "source": [
        "To confirm it works with our batched tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UshHkAps5x9c",
        "outputId": "e4992be0-5372-4e1c-99bd-2a633f8a65e9"
      },
      "outputs": [],
      "source": [
        "test_tensor = torch.tensor([[[1, 2, 3, 4, 5], [5, 6, 7, 8222, 10]],\n",
        "                            [[1, 2, 22223, 4, 5], [5, 6, 7, 8, 10]]], dtype=torch.float32)\n",
        "ffl = FeedForwardLayer(5, 15)\n",
        "ffl(test_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gaurNng7XR6"
      },
      "source": [
        "### Exercise 8: Building the (Encoder) Attention Layer\n",
        "\n",
        "With all our components in place, we can build a single-head attention layer.\n",
        "\n",
        "We need two more things:\n",
        "\n",
        "\n",
        "*   First, our attention head's embedding dimension is `dim_k`, whereas the rest of our transformer uses `dim_h`. This is because we might want to have several parallel attention heads. We are thus missing the aggregation step, with a final linear layer ensuring the correct dimension.\n",
        "\n",
        "*   Second, recall that we want to add a skip connection and - depending on how we do it - one or two layernorms.\n",
        "Here, we will implement the Pre-Normalization skip connections, which have been shown to make training easier.\n",
        "\n",
        "See, if you can fill the missing parts in the module skeleton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av0ptmLy538d"
      },
      "outputs": [],
      "source": [
        "class SimpleAttentionLayer(nn.Module):\n",
        "  def __init__(self, dim_h, dim_k, dim_expanded):\n",
        "    super().__init__()\n",
        "\n",
        "    # Attention head\n",
        "    self.qkv = QueryKeyValue(dim_h, dim_k)\n",
        "    self.dot_product_attn = ScaledDotProduct()\n",
        "    self.aggregation_step = nn.Linear(dim_k, dim_h)\n",
        "\n",
        "    # Layer Norm(s)\n",
        "    self.layernorm1 = nn.LayerNorm(dim_h)\n",
        "    self.layernorm2 = nn.LayerNorm(dim_h)\n",
        "    \n",
        "    # Feedforward Layer\n",
        "    self.ffl = FeedForwardLayer(dim_h,dim_expanded)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Branch off from the skip connection before the initial layernorm\n",
        "    # - that is: we keep x as is\n",
        "    h = self.layernorm1(x)\n",
        "    \n",
        "    # Triplicate our input matrix\n",
        "    query, key, value = self.qkv(h)\n",
        "    \n",
        "    # Run the scaled dot product attention operation\n",
        "    h = self.dot_product_attn(query, key, value)\n",
        "    h = self.aggregation_step(h)\n",
        "    \n",
        "    # Our first skip connection returns to the main path\n",
        "    x = h + x\n",
        "    \n",
        "    # Branch off again with the second skip leading into the feedforward layer\n",
        "    h = self.layernorm2(x)\n",
        "    h = self.ffl(h)\n",
        "    \n",
        "    # Finally, return the skip connection\n",
        "    x = h + x\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSrsyC8e55ku"
      },
      "outputs": [],
      "source": [
        "# Test the attention layer\n",
        "attn_layer = SimpleAttentionLayer(5, 5, 15)\n",
        "attn_layer(test_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpA1rre7_uqR"
      },
      "source": [
        "### What is missing?\n",
        "\n",
        "Actually - not much. With the addition of a head - say a softmax classification head, and token+positional embeddings, this is a working transformer.\n",
        "\n",
        "We have not implemented a dropout layer, which would go at the end - after the feedforward layer . If you use it, you'll want to differentiate between evaluation and training and disable the dropout for the former.\n",
        "\n",
        "Second, we are missing multi-headed attention. But: Having specified a single attention head, that's easy to do. Try it yourself!\n",
        "\n",
        "Hint: Use a PyTorch Module List\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pro Version\n",
        "\n",
        "If you code NN in Python, you should be keenly aware of torch.einsum and einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from einops import rearrange\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_input: int,\n",
        "        nr_heads: int = 8,\n",
        "        dim_head: int = 16,\n",
        "        dropout_p: float = 0.0,\n",
        "        scale_factor: float = 0.5,\n",
        "    ):\n",
        "        \"\"\"Ye olde Multihead Attention, implmented with Einstein Notation.\n",
        "        Note: There' ain't no masking here\n",
        "\n",
        "        Args:\n",
        "            dim_input (int): The input dimension\n",
        "            nr_heads (int, optional): Number of heads. Defaults to 8.\n",
        "            dim_head (int, optional): Dimension of heads. Defaults to 16.\n",
        "            dropout_p (float, optional): Dropout. Defaults to 0.0.\n",
        "            scale_factor (float, optional): Exponent of the scaling division - default is square root. Defaults to 0.5.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.nr_heads = nr_heads\n",
        "        self.scale = dim_head**-scale_factor\n",
        "\n",
        "        self.to_qkv = torch.nn.Linear(dim_input, dim_head * nr_heads * 3, bias=False)\n",
        "        self.to_out = torch.nn.Linear(dim_head * nr_heads, dim_input)\n",
        "        self.dropout = torch.nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.nr_heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n",
        "        sim = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
        "\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\", h=h)\n",
        "        return self.to_out(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8By7yUxHLkgw"
      ],
      "name": "attention_from_scratch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('nlp-training')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "c662d7cd23b6de128b6d84794298b91bf8fa078dbbac08b1f8f98d16f1457de4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
