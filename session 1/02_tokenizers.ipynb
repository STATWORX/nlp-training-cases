{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Training 2: Tokenizers\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting working dir to: /Users/ingomarquart/Documents/GitHub/itern-nlp-training-cases\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "print(f'Setting working dir to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "In this section, we will tokenize a text using different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: One-Hot Encodings\n",
    "\n",
    "Tokenize the sentence below by splitting by world, then transform it to an one-hot-encoding representation.\n",
    "\n",
    "You can use either of two tools:\n",
    "1. Use the `OneHotEncoder` from `sklearn` to create the representation.\n",
    "\n",
    "2. You can use `one_hot` from `torch.nn.functional`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_text = 'Do you think, large language models are slightly conscious?'\n",
    "\n",
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. First, you need to split the string into individual elements. You can use nltk.tokenize.word_tokenize\n",
    "\n",
    "2. Second, to use sklearn, we need to define the number of unique tokens \n",
    "\n",
    "2. Use the sklearn OneHotEncoder, remember that we do not want a sparse representation in this case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token representation \n",
      " ['Do', 'you', 'think', ',', 'large', 'language', 'models', 'are', 'slightly', 'conscious', '?']\n",
      "ID representation \n",
      " [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Sklearn: One-hot representation \n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "PyTorch: One-hot representation \n",
      " tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(your_text)\n",
    "\n",
    "# Alternative 1: Use String Functions and regex\n",
    "# import re\n",
    "# tokens = your_text.split(' ')\n",
    "# tokens = [re.split('(\\W)', token)[0:2] if any(p in token for p in string.punctuation) else token for token in tokens]\n",
    "# tokens = flatten(tokens)\n",
    "\n",
    "# Alternative 2: RegEx Ninja skills\n",
    "# tokens = re.findall(r\"[\\w']+|[.,!?;]\", your_text)\n",
    "\n",
    "print(f'Token representation \\n {tokens}')\n",
    "\n",
    "ids = np.arange(0, len(tokens))\n",
    "print(f'ID representation \\n {ids}')\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(ids.reshape(-1, 1))\n",
    "print(f'Sklearn: One-hot representation \\n {onehot_encoded}')\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "one_hot_encodings_torch = F.one_hot(ids_torch, num_classes=len(set(ids)))\n",
    "print(f'PyTorch: One-hot representation \\n {one_hot_encodings_torch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the issues with the above approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: PyTorch Transformer Tokenizers\n",
    "\n",
    "To solve the issues, we will now a tokenizer from PyTorch Transformers.\n",
    "\n",
    "Production-ready tokenizers differ from the above in a number of ways: They are subword tokenizers, are adapted to a particular model, and are trained on a corpus to have a larger vocabulary.\n",
    "Tokenizers also add additional information required by the model. We will have a more detailed look at this in session 2!\n",
    "\n",
    "In this task, use PyTorch Transformers to load a tokenizer for a Bert-type model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_text = 'Do you think, large language models training statworks are slightly conscious?'\n",
    "model = \"gpt2\"\n",
    "\n",
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. You can use `AutoTokenizer` to get the correct tokenizer for your model (identified by string)\n",
    "\n",
    "2. `AutoTokenizer` has the `from_pretrained` function to load a tokenizer model from the Hugginface Hub\n",
    "\n",
    "3. The tokenizer has different functions to tokenizer, encode and convert text. Try to understand the differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"manual\" way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', 'Ġyou', 'Ġthink', ',', 'Ġlarge', 'Ġlanguage', 'Ġmodels', 'Ġtraining', 'Ġstat', 'works', 'Ġare', 'Ġslightly', 'Ġconscious', '?']\n",
      "[5211, 345, 892, 11, 1588, 3303, 4981, 3047, 1185, 5225, 389, 4622, 6921, 30]\n",
      "['Do', 'Ġyou', 'Ġthink', ',', 'Ġlarge', 'Ġlanguage', 'Ġmodels', 'Ġtraining', 'Ġstat', 'works', 'Ġare', 'Ġslightly', 'Ġconscious', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# Use transformers\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# We can use the tokenizer's tokenize function for the first step\n",
    "tokens = transformer_tokenizer.tokenize(your_text)\n",
    "print(tokens)\n",
    "# Now we encode the tokens with the tokenizer's encode_plus function\n",
    "# Since we are using PyTorch, we will return PyTorch tensors\n",
    "token_ids = transformer_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n",
    "# To check that this did what we wanted, we can use the tokenizer's decode function\n",
    "print(transformer_tokenizer.convert_ids_to_tokens(token_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers do more: They encode the text by adding the necessary special tokens for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5211, 345, 892, 11, 1588, 3303, 4981, 3047, 1185, 5225, 389, 4622, 6921, 30]\n",
      "['Do', 'Ġyou', 'Ġthink', ',', 'Ġlarge', 'Ġlanguage', 'Ġmodels', 'Ġtraining', 'Ġstat', 'works', 'Ġare', 'Ġslightly', 'Ġconscious', '?']\n"
     ]
    }
   ],
   "source": [
    "# Use the encode function in one go\n",
    "token_ids = transformer_tokenizer.encode(your_text)\n",
    "print(token_ids)\n",
    "print(transformer_tokenizer.convert_ids_to_tokens(token_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that Tokenizers should actually be called via the forward function.\n",
    "\n",
    "In addition to encoding the sentence, it also adds other information required by the model - such as the attention mask (see session 2!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5211, 345, 892, 11, 1588, 3303, 4981, 3047, 1185, 5225, 389, 4622, 6921, 30], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Regular way: Use the forward function of the tokenizer\n",
    "encoded_sentence = transformer_tokenizer(your_text)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use PyTorch, we can even return PyTorch tensors like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[5211,  345,  892,   11, 1588, 3303, 4981, 3047, 1185, 5225,  389, 4622,\n",
      "         6921,   30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_sentence = transformer_tokenizer(your_text, return_tensors='pt')\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Tokenizer was from the famous BERT model.\n",
    "\n",
    "Try `model = \"gpt2\"`and see what the differences are!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Batch Encoding with Transformers\n",
    "\n",
    "We typically want to encode a whole bunch of sentences.\n",
    "Either, since we plan to apply tokenization as a map, or because we want to prepare a whole "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/ingomarquart/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /Users/ingomarquart/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d6ba27cc6c67cf9a.arrow\n",
      "Loading cached processed dataset at /Users/ingomarquart/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5edef9c02646c5f2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In 1969 , General António Augusto dos Santos was relieved of command , with General Kaúlza de Arriaga taking over officially in March 1970 . Kaúlza de Arriaga favoured a more direct method of fighting the insurgents , and the established policy of using African counter @-@ insurgency forces was rejected in favour of the deployment of regular Portuguese forces accompanied by a small number of African fighters . Indigenous personnel were still recruited for special operations , such as the Special Groups of Parachutists in 1973 , though their role less significant under the new commander . His tactics were partially influenced by a meeting with United States General William Westmoreland . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, list_datasets\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "dataset = dataset.filter(lambda x: x['text'] != '')\n",
    "dataset = dataset.filter(lambda x: ~x['text'].startswith('='))\n",
    "idx = np.random.randint(0, len(dataset))\n",
    "random_sentence = dataset['text'][idx]\n",
    "print(random_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_range = slice(1602,1605)\n",
    "text=dataset['text'][idx_range]\n",
    "model = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your exercise is to tokenize these sentences and collect their token_ids in a single batched PyTorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the same tokenizer as before\n",
    "\n",
    "2. Think about the output shape of each sentence, and what dimension your PyTorch tensor will have\n",
    "\n",
    "3. Check out the parameters of the forward function of your tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Adam Stansfield ( 10 September 1978 – 10 August 2010 ) was an English professional footballer who played as a striker . He competed professionally for Yeovil Town , Hereford United and Exeter City , and won promotion from the Football Conference to The Football League with all three teams . \\n',\n",
       " \" Having played for three counties as a child , Stansfield began his career in non @-@ league with Cullompton Rangers and Elmore , and had unsuccessful trials at league teams . At the age of 23 , he signed his first professional contract with Yeovil Town , after impressing their manager Gary Johnson in a match against them . In his first season , he helped them win the FA Trophy , scoring in the 2002 final . The following season , Yeovil won the Conference and promotion into The Football League , although Stansfield was ruled out with a broken leg in the first game . In 2004 , he transferred to Hereford United , where he won promotion to The Football League via the 2006 play @-@ offs , and repeated the feat with Exeter City a year later . He also helped Exeter earn promotion into League One in 2008 . At international level , Stansfield played five matches and scored one goal for England 's national semi @-@ professional team , winning the 2005 Four Nations Tournament . \\n\",\n",
       " ' Stansfield was diagnosed with colorectal cancer in April 2010 . He returned to training after surgery and chemotherapy , but died on 10 August that year . A foundation in his name was posthumously set up by his family to provide sporting opportunities and raise awareness of colorectal cancer . He has posthumously been featured on a Flybe airliner livery and tourist currency in Exeter . \\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text pieces have very different length. If we simply use the Tokenizer, we will get a helpful error telling us that this won't work: PyTorch tensors have a fixed dimension.\n",
    "\n",
    "Luckily, our Tokenizer also has the option to pad and truncate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "encoded_batch = transformer_tokenizer(text, return_tensors='pt', padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 114])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1556,  1103,  1207, 17452,   117,  1126,  5677,  1108,  2234,\n",
       "          1154,  2689,  1555,   119,  1109,  1353,  7309,  1104,  6133,  4384,\n",
       "          1276,  1142,  1849,  1106,  1129,  1154,  2879,  1895,   117,  1105,\n",
       "          1152,  1310,  1106,  2080,  1147,  1826, 10380,   117,  1107, 12765,\n",
       "          4045,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1130,  1103,  6148,  8386,   117,  1103, 17452,  3421,  3137,\n",
       "          3290,  1105,  1108, 14129,  1103,  1378,  1214,   119,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101, 10236,  1193,  1496,  1106,  9027,  1121,  5169,   117, 17984,\n",
       "          1105, 12556,  1611,  7137,   117,  1103,  2778,  1416,  1104, 13203,\n",
       "          1976,  2580,  1107,  2060,   131,  1121,   122,   137,   117,   137,\n",
       "         24053,  1484,  1107,  6225,  1106,   124,   137,   117,   137, 25950,\n",
       "          1484,  1107,  4337,   117,  1105,  1173,  1106,   126,   137,   117,\n",
       "           137,  5311,  1568,  1484,  1107,  4085,   119,  1109, 17452,  1245,\n",
       "          1315,  1353,  1106,  8378,  1103,  2993,  1104,  1103,  1518,   137,\n",
       "           118,   137,  2898,  1661,   119,  1130,  4085,   170, 10808,  1108,\n",
       "         10510,  1106,  2773,  1103,  1295,  1104,  1907,  3474,   119,   138,\n",
       "          3916,  2197,  1106,  2773,  1103,  3211,  1106,  5706,  1527,  3474,\n",
       "          1108,  4444,  3928,   119,   138,  2129, 11187,  1449,  1108,  4631,\n",
       "          1107,  3698,   119,   102]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoded_batch[\"input_ids\"].shape)\n",
    "encoded_batch[\"input_ids\"][:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Tokenizing and Batching for a fixed length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior code allows you to tokenize a batch of inputs to its longest example.\n",
    "This leads to dynamic batch sizes (more on this below).\n",
    "\n",
    "Sadly, dynamic batches are not supported in every situation. For instance, it doesn't work on TPUs. \n",
    "\n",
    "Or, a case more relevant to us, we might not have enough memory to deal with the very largest of sentences. Since sentence length follows a power law, these are very few. But they love to crash your pipeline at the end of an epoch!\n",
    "\n",
    "\n",
    "Your task in this exercise is to create, for the same text above, a PyTorch batch of tokenized sentences - using padding and truncation - to length 35\n",
    "\n",
    "As additional challenge, use the tokenizer for GPT2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/ingomarquart/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /Users/ingomarquart/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-d6ba27cc6c67cf9a.arrow\n",
      "Loading cached processed dataset at /Users/ingomarquart/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-5edef9c02646c5f2.arrow\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, list_datasets\n",
    "\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "dataset = dataset.filter(lambda x: x['text'] != '')\n",
    "dataset = dataset.filter(lambda x: ~x['text'].startswith('='))\n",
    "idx_range = slice(1602,1605)\n",
    "text=dataset['text'][idx_range]\n",
    "model = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT2 tokenizer has no padding token by default. This is because it is an auto-regressive model (more in session 2). Here, we need to set the padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is appropriate for GPT2\n",
    "transformer_tokenizer.pad_token = transformer_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use set the max_length parameter to 35. If we use the padding strategy \"max_length\" and the truncation strategy \"longest_first\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_batch = transformer_tokenizer(text, return_tensors='pt', padding=\"max_length\", max_length=35, truncation=\"longest_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 35])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7244,   520,   504,  3245,   357,   838,  2693, 15524,   784,   838,\n",
       "          2932,  3050,  1267,   373,   281,  3594,  4708, 44185,   508,  2826,\n",
       "           355,   257, 19099,   764,   679, 32440, 28049,   329, 11609,   709,\n",
       "           346,  8329,   837,  3423,  3841],\n",
       "        [11136,  2826,   329,  1115, 14683,   355,   257,  1200,   837,   520,\n",
       "           504,  3245,  2540,   465,  3451,   287,  1729,  2488,    12,    31,\n",
       "          4652,   351, 31289,   296, 10972, 13804,   290,  2574,  3549,   837,\n",
       "           290,   550, 23993,  9867,   379],\n",
       "        [  520,   504,  3245,   373, 14641,   351,   951,   382,   310,   282,\n",
       "          4890,   287,  3035,  3050,   764,   679,  4504,   284,  3047,   706,\n",
       "          8185,   290, 34696,   837,   475,  3724,   319,   838,  2932,   326,\n",
       "           614,   764,   317,  8489,   287]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoded_batch[\"input_ids\"].shape)\n",
    "encoded_batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Tokenizing in the PyTorch Pipeline and in the Transformers (Huggingface) pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Tokenizing the whole dataset using the PyTorch Transformer Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 Tokenize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the standard case, we will apply the tokenizer to our dataset.\n",
    "\n",
    "When using a Transformer dataset, we can simply use the map function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/ingomarquart/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /Users/ingomarquart/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-0eba9b53d2ecc055.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, list_datasets\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "from transformers import AutoTokenizer\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# This is lazily evaluated!\n",
    "new_dataset=dataset.map(lambda x: transformer_tokenizer(x[\"text\"]))\n",
    "new_dataset=new_dataset.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Use a Hugginface DataCollator for your Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use the Hugginface trainer interface, you can use a DataCollator to automize batching and other collation operations.\n",
    "These include standard masked-language modeling, or alignment operations for language modeling\n",
    "\n",
    "In this case, it's best to provide a DataCollator to the trainer class.\n",
    "The trainer will then use dynamic batching, but also align training examples to be similar in size to minimize padding and maximize throughput (etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "datacollator = DataCollatorForLanguageModeling(tokenizer=transformer_tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this works in the trainer, we have to use a trick. The trainer, our code, or the PyTorch Dataset will encode examples via the tokenizer. The collator receives these as iterable and will create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List_of_encodings_to_batch=[new_dataset[15],new_dataset[16],new_dataset[17]]\n",
    "type(List_of_encodings_to_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the model includes our sequences with randomly-masked tokens and a new \"label\" tensor, that includes the true values of the masked tokens\n",
    "\n",
    "**We have not used padding during tokenization, because the datacollator performs it and, if used in the trainer class, will apply further optimizations**\n",
    "\n",
    "\n",
    "The masking Token for BERT is 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1109,  1342,   103,  1282,  1219,  1103,  2307,   103,   103,\n",
      "          1414,   119,   144,  5727,  1811],\n",
      "        [  101,  1249,   103, 10208,  2008,  3184,  1202,   103,  9933,   103,\n",
      "          1103,   103,   174, 20492,  4199],\n",
      "        [  101,   103,  1193,  1496,   103,  1292,  1958,   117,  1105,  6146,\n",
      "          1496,  1106,  1103,  1558,  6053]])\n",
      "tensor([[ -100,  1109,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100],\n",
      "        [ -100,  -100,  1103, 10208,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  3105,   174,  -100,  4199],\n",
      "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1105,  6146,\n",
      "          -100,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "print(datacollator(List_of_encodings_to_batch).input_ids[:,:15])\n",
    "print(datacollator(List_of_encodings_to_batch).labels[:,:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Tokenizing for a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a PyTorch Dataset, we have two options:\n",
    "\n",
    "1. Tokenize the entire dataset during initialization.\n",
    "\n",
    "    This is great if our dataset fits in memory, or if we have access to fast storage such that we can save the prepared dataset (or cache it otherwise). \n",
    "\n",
    "    We can then either: \n",
    "    \n",
    "    * Use a custom collate_fn to do the batching dynamically\n",
    "\n",
    "    * Pad and truncate all examples to a fixed length and use standard PyTorch collation\n",
    "\n",
    "2. Tokenize on the fly\n",
    "\n",
    "    This might be required if we can not load the data before (e.g. streaming). \n",
    "    \n",
    "    Another case where this happens if the training examples themselves have several examples combined in a complicated manner - for example if the masking probabilities depend on the properties of the batch, or when a whole set of examples are augmented from a single example.\n",
    "\n",
    "    In this case we'd return or yield these batches, including padding and tokenization, right from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp-training')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c662d7cd23b6de128b6d84794298b91bf8fa078dbbac08b1f8f98d16f1457de4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
