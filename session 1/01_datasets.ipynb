{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Training 1: Datasets\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting working dir to: /Users/ingomarquart/Documents/GitHub/itern-nlp-training-cases\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "print(f'Setting working dir to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Datasets\n",
    "\n",
    "Get a list of all datasets available on the Huggingface Hub.   \n",
    "How many datasets are available?   \n",
    "Take a closer look at the first five ones.   \n",
    "\n",
    "There is a dataset called [\"emotion\"](https://huggingface.co/datasets/emotion).   \n",
    "\n",
    "1. Load it and take a look at the column names and features.   \n",
    "2. Take a look at the first row of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8831\n",
      "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news']\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017279863357543945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": 1655,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de370f4aeec54224a229950bb0fd0748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008041858673095703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading metadata",
       "rate": null,
       "total": 1611,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b6387e8927458e9fe915bb08e5a5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /Users/ingomarquart/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01749706268310547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 1658616,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f84a98984949b2b39d0fd9d726b357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, list_datasets\n",
    "\n",
    "# Explore all datasets\n",
    "all_datasets = list_datasets()\n",
    "print(len(all_datasets))\n",
    "print(all_datasets[:6])\n",
    "\n",
    "# Load the emotion dataset\n",
    "dataset = load_dataset(\"emotion\") \n",
    "train_dataset = dataset['train']\n",
    "\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(train_dataset.column_names)\n",
    "print(train_dataset.features)\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 a\n",
    "\n",
    "Convert the dataset to pandas and print its head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='pandas')\n",
    "df = dataset['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple NLP pipeline in a few simple steps\n",
    "\n",
    "Create an inference pipeline that creates a summary up to 50 words long of the following text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. \n",
    "In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; \n",
    "the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. \n",
    "Importantly, a deep learning process can learn which features to optimally place in which level on its own. \n",
    "This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.\n",
    "The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. \n",
    "More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. \n",
    "CAPs describe potentially causal connections between input and output. \n",
    "For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). \n",
    "For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.\n",
    "No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. \n",
    "CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function. \n",
    "Beyond that, more layers do not add to the function approximator ability of the network. \n",
    "Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\n",
    "\"\"\"\n",
    "\n",
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline('summarization')\n",
    "summarizer(text, min_length=5, max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the same but this time using [Google's Pegasus model](https://huggingface.co/google/pegasus-xsum).   \n",
    "How does the output differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'google/pegasus-xsum'\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=model)\n",
    "summarizer(text, min_length=5, max_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Dataset Schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Map Dataset: Offloading complexity to workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Do something here\n",
    "    return data\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform, augmentation):\n",
    "        self.data = data\n",
    "        self.data = preprocess_data(self.data)\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        observation = self.data.iloc[index,:]\n",
    "        observation = torch.tensor(observation, dtype=torch.float)\n",
    "        observation = self.augmentation(observation, self.transform)\n",
    "        features, label = observation[:-1], observation[-1]\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Iterable Dataset: For streaming data, big data, data too big to be preloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "from Petastorm import make_batch_reader\n",
    "\n",
    "class CustomIterableDataset(IterableDataset):\n",
    "    def __init__(self, data_files, epoch_length,\n",
    "                                        cur_shard, num_shards):\n",
    "        self.parquet_files = data_files\n",
    "        self.reader = make_batch_reader(data_files, num_epochs=None, \n",
    "                            cur_shard=cur_shard, num_shards=num_shards)\n",
    "        self.epoch_length = epoch_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i,observation in enumerate(self.reader):\n",
    "            if i == self.epoch_length:\n",
    "                print(\"Epoch completed\")\n",
    "                break\n",
    "            observation = torch.tensor(observation, dtype=torch.float) \n",
    "            features, label = observation[:-1], observation[-1]\n",
    "            yield features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp-training')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c662d7cd23b6de128b6d84794298b91bf8fa078dbbac08b1f8f98d16f1457de4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
