{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Training 4: Word2Vec (in PyTorch) from scratch\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "print(f'Setting working dir to: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from utils.word2vec import get_dataloader_and_vocab\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch and Word2Vec\n",
    "\n",
    "In this notebook, we are going to create a simple version of the original Word2Vec model in PyTorch.    \n",
    "The implementation follows [this blog post](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0) from medium. Make sure not to take a solution before you tried to solve it by yourself ðŸ˜‰.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Data Loader\n",
    "\n",
    "We already implemented, imported and instantiated a [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html) class and a [PyTorchText Vocablurary](https://pytorch.org/text/stable/vocab.html#id1) class for you (see below).   \n",
    "\n",
    "The Vocab class is a simple mapping from Words to token IDs. It will do the following:\n",
    "\n",
    "1. Create a vocabulary of a particular size and store it\n",
    "\n",
    "The DataLoader will draw batches of samples from the WikiText2 dataset that we will later use to train the Word2Vec model. It will do the follow steps:\n",
    "\n",
    "1. Take a paragraph from the raw datafile\n",
    "2. Convert it to lowercase, tokenize, and encode it\n",
    "3. Make sure paragraph are neither to long nor to short\n",
    "4. Transform the paragraph into context and target words using a moving window\n",
    "5. Return both context (input) and target (output) as a batch\n",
    "\n",
    "The DataLoader is a Python Iterable (implements the `__iter__()`-method).   \n",
    "\n",
    "Try to do the following:   \n",
    "- Check the length of the vocablurary we created\n",
    "- Can you find out, which token ID (aka index) was a assigned to the word \"deep\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader, vocab = get_dataloader_and_vocab(\n",
    "        ds_name='WikiText2',\n",
    "        ds_type='train',\n",
    "        batch_size=4,\n",
    "        n_window=4,\n",
    "        shuffle=True,\n",
    "        vocab=None)\n",
    "\n",
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_vocab = len(vocab)\n",
    "print(f'Used a vocablurary of length: {len_of_vocab}')\n",
    "\n",
    "idx_of_deep = vocab.lookup_indices(['deep'])\n",
    "print(f'The index of the word \"deep\" is: {idx_of_deep}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1a\n",
    "\n",
    "Next, do the following:\n",
    "- Draw a single batch from the `dataloader`\n",
    "- What does it return, what is the size?\n",
    "- Check the correctness of the input and output by converting the token indices back to raw tokens and thereby reconstruct the original sentence (for simplicity use only the first sample of the batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output = next(iter(dataloader))\n",
    "\n",
    "print(f'Input shape: {input.shape}')\n",
    "print(f'Output shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_input_idx = input[0].numpy()\n",
    "first_output_idx = output[0].numpy()\n",
    "\n",
    "first_input_tokens = vocab.lookup_tokens(first_input_idx)\n",
    "first_output_tokens = vocab.lookup_tokens([first_output_idx])\n",
    "\n",
    "print(f'Left context: {first_input_tokens[:4]}, Target: {first_output_tokens}, Right context: {first_input_tokens[4:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Implementing the Word2Vec CBOW Model\n",
    "\n",
    "In the following exercise, you should complete the given code for the Word2Vec CBOW model. We already implemented most of the components for a [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html) model class.   \n",
    "\n",
    "See if you can fill in the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here:\n",
    "# -> SEE CAPITAL LETTERS\n",
    "\n",
    "class CBOW_Model(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Implementation of CBOW model described in paper:\n",
    "    https://arxiv.org/abs/1301.3781\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 emed_dim: int = 300, \n",
    "                 embed_max_norm: int = 1):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        \n",
    "        # We need to define embedding layer first, see if you can figure out\n",
    "        # how to do it.\n",
    "        # Hint: PyTorch offers a special embedding layer that can handle this\n",
    "        self.embeddings = # FILL IN HERE\n",
    "        \n",
    "        # Then we need to define linear layer.\n",
    "        self.linear = # FILL IN HERE\n",
    "        \n",
    "        # Finally, we need to define loss function.\n",
    "        self.loss = \n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        \"\"\"\n",
    "        Forward pass of CBOW model.\n",
    "\n",
    "        Be aware, no softmax activation in output due \n",
    "        to the PyTorch CrossEntropyLoss requiring raw unnormalized scores.\n",
    "\n",
    "        Args:\n",
    "            inputs_: tensor of shape (batch_size, n_window*2)\n",
    "                     where n_window is the number of context words.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we need to get embeddings of all context words by passing\n",
    "        # them through embedding layer.\n",
    "        x = # FILL IN HERE\n",
    "        \n",
    "        # Then we need to average all embeddings.\n",
    "        x = x.mean(axis=1)\n",
    "        \n",
    "        # Finally, we need to pass averaged embeddings through linear layer.\n",
    "        x = # FILL IN HERE\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step of CBOW model.\n",
    "\n",
    "        Args:\n",
    "            batch: tuple of (inputs, targets)\n",
    "            batch_idx: index of batch\n",
    "        \"\"\"\n",
    "        # Get inputs and targets from batch.\n",
    "        x, y = batch\n",
    "        \n",
    "        # Forward pass will return raw logits\n",
    "        logits = self.forward(x)\n",
    "        \n",
    "        # We need to calculate loss for each batch\n",
    "        loss = # FILL IN HERE\n",
    "        \n",
    "        # We need to log the loss for each batch\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step of CBOW model.\n",
    "        \n",
    "        Args:\n",
    "            val_batch: tuple of (inputs, targets)\n",
    "            batch_idx: index of batch\n",
    "        \"\"\"\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers for CBOW model.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW_Model(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Implementation of CBOW model described in paper:\n",
    "    https://arxiv.org/abs/1301.3781\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 emed_dim: int = 300, \n",
    "                 embed_max_norm: int = 1):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emed_dim,\n",
    "            max_norm=embed_max_norm,\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            in_features=emed_dim,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        \"\"\"\n",
    "        Forward pass of CBOW model.\n",
    "\n",
    "        Be aware, no softmax activation in output due \n",
    "        to the PyTorch CrossEntropyLoss requiring raw unnormalized scores.\n",
    "\n",
    "        Args:\n",
    "            inputs_: tensor of shape (batch_size, n_window*2)\n",
    "                     where n_window is the number of context words.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step of CBOW model.\n",
    "\n",
    "        Args:\n",
    "            batch: tuple of (inputs, targets)\n",
    "            batch_idx: index of batch\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step of CBOW model.\n",
    "        \n",
    "        Args:\n",
    "            val_batch: tuple of (inputs, targets)\n",
    "            batch_idx: index of batch\n",
    "        \"\"\"\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizers for CBOW model.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "DATASET = 'WikiText2'\n",
    "BATCH_SIZE = 128\n",
    "N_WINDOW = 4\n",
    "\n",
    "# Data sets\n",
    "train_dataloader, vocab = get_dataloader_and_vocab(\n",
    "        ds_name=DATASET,\n",
    "        ds_type='train',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_window=N_WINDOW,\n",
    "        shuffle=True,\n",
    "        vocab=None,\n",
    "    )\n",
    "\n",
    "val_dataloader, _ = get_dataloader_and_vocab(\n",
    "        ds_name=DATASET,\n",
    "        ds_type='valid',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_window=N_WINDOW,\n",
    "        shuffle=False,\n",
    "        vocab=vocab,\n",
    "    )\n",
    "\n",
    "# Init model\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "model = CBOW_Model(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "\n",
    "# Determine device\n",
    "device = torch.device('gpu' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "trainer = pl.Trainer(accelerator=device,\n",
    "                     max_epochs=5,\n",
    "                     callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Get Embeddings and Calculate Similarity\n",
    "\n",
    "Finally, we need to extract the embeddings from the linear layer. Each row in the embedding corresponds to a word in our vocablurary.   \n",
    "\n",
    "- Can you proof that the embeddings have to correct shape?\n",
    "- See how good the model performes by calculating the similarity for the word \"mother\" with all other words in our vocablurary and find the five most similar ones (use the `utils.word2vec.get_top_similar` function for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.word2vec import get_top_similar\n",
    "\n",
    "# Add your solution here:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings from the linear layer of our model\n",
    "w2v_embeddings = model.linear.weight.detach().cpu().numpy()\n",
    "\n",
    "# Each row is a word embedding\n",
    "print(f'Shape of embeddings: {w2v_embeddings.shape} and vocab size: {vocab_size}')\n",
    "\n",
    "# If we would like, we could get the corresponding words from our vocablurary\n",
    "# print(vocab.get_itos())\n",
    "\n",
    "get_top_similar(w2v_embeddings, 'mother', vocab, top_n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
